{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F18NIj40a0io"
      },
      "source": [
        "### define drive & paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO-CoYTUa6Mc",
        "outputId": "9d9c2213-ebb8-49fe-dca7-5c8d771bbf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irm5YexvakcI"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = \"/content/drive/MyDrive/AraLlamaProject/\"\n",
        "PRETRAIND_MODEL = \"/content/drive/MyDrive/AraLlamaProject/Models_1/\"\n",
        "TOKENIZERS = \"/content/drive/MyDrive/AraLlamaProject/tokenizers_1/\"\n",
        "FINTUNED_MODEL = \"/content/drive/MyDrive/AraLlamaProject/Fine-tuning-models/\"\n",
        "DATASETS_PATH = \"/content/drive/MyDrive/AraLlamaProject/Datasets/\"\n",
        "tasks = [\"Arabic-Natural-Language-Inference\",\"Arabic-Hate-Speech\",\"Arabic-News-Articles\",\"Arabic-Sentiment-Analysis\"]\n",
        "vocab_sizes = [16000,28000,44000]\n",
        "token_methods = [\"BPE\",\"WPC\", \"WLV\"]\n",
        "is_farasa = [\"with-farasa\",\"nofarasa\"]\n",
        "ARGS = [(3,50),(5,50),(10,50),(10,50),(10, 50)] #arguments for the finetunning (num_epochs , batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAOtc-peba4D"
      },
      "source": [
        "### load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WEHrzAxbWS5"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet datasets sentencepiece argparse &> /dev/null\n",
        "!pip install --quiet git+https://github.com/huggingface/transformers@v4.28.1 &> /dev/null\n",
        "!pip install --quiet evaluate &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PvjzFIubfHO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "import io\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
        "                                WordPieceTrainer, UnigramTrainer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, Trainer, PreTrainedTokenizerFast\n",
        "from transformers import Trainer, TrainingArguments, pipeline\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
        "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
        "import os\n",
        "import json\n",
        "from datasets import load_dataset,load_from_disk\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from datasets import DatasetDict\n",
        "from transformers import TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuUY4ZQtdScc"
      },
      "source": [
        "### functionalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lOqmaspdR3W"
      },
      "outputs": [],
      "source": [
        "def load_datasets(task):\n",
        "  # print(f'LOADING DATASET :{task}')\n",
        "  dataset = load_from_disk(DATASETS_PATH + '{}/hugging-face'.format(task))\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXVGy2sgdtVY"
      },
      "outputs": [],
      "source": [
        "def load_model_tokenizer(model_path, token_path,num_label):\n",
        "  # ===================================================\n",
        "  tokenizer = Tokenizer.from_file(token_path)\n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,pad_token = \"<pad>\")\n",
        "  # ===================================================\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_label)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "  return model , tokenizer , data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvvUvHj2e6RG"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,max_length=512)\n",
        "def tokenize_function_2(examples):\n",
        "  return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", truncation=True,max_length=512)\n",
        "# ==============================================================================\n",
        "def tokenization(dataset,task):\n",
        "  if task == \"Arabic-Natural-Language-Inference\":\n",
        "    tokenized_dataset = dataset.map(tokenize_function_2, batched=True)\n",
        "    tokenized_dataset = tokenized_dataset.remove_columns([\"premise\",\"hypothesis\"])\n",
        "    tokenized_dataset = tokenized_dataset.rename_column('label','labels')\n",
        "    tokenized_dataset.with_format('pt')\n",
        "    return tokenized_dataset\n",
        "  tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "  tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "  tokenized_dataset = tokenized_dataset.rename_column('label','labels')\n",
        "  tokenized_dataset.with_format('pt')\n",
        "  return tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cnqr6d9fB8j"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset):\n",
        "  train_testvalid = dataset.train_test_split(test_size=0.2)\n",
        "  test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "  train_test_valid_dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})\n",
        "  return train_test_valid_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZXjUFv9iY-w"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "  'accuracy': acc,\n",
        "  'f1': f1,\n",
        "  'precision': precision,\n",
        "  'recall': recall\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KONmBLV6hfhl"
      },
      "outputs": [],
      "source": [
        "def load_args(num_epochs , Batch_size):\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=\"./test_trainer\",\n",
        "      num_train_epochs=num_epochs,\n",
        "      gradient_accumulation_steps=1,\n",
        "      per_device_train_batch_size=Batch_size,\n",
        "      fp16=True,\n",
        "      adam_beta1=0.90,\n",
        "      adam_beta2 =0.98,\n",
        "      adam_epsilon = 1e-6,\n",
        "      learning_rate=1e-5\n",
        "      )\n",
        "  return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iVd_M-oigPC"
      },
      "outputs": [],
      "source": [
        "def load_trainer(model ,training_args ,data_collator , dataset ):\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=dataset['train'],\n",
        "      eval_dataset=dataset['valid'],\n",
        "      compute_metrics=compute_metrics,\n",
        "      data_collator=data_collator\n",
        "  )\n",
        "  return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTTX4ig1i6IL"
      },
      "outputs": [],
      "source": [
        "def predict(dataset,trainer):\n",
        "  dataset = dataset['test'].remove_columns(['labels'])\n",
        "  yhat = trainer.predict(dataset)\n",
        "  preds = np.argmax(yhat.predictions, axis=1)\n",
        "  return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyOTu1bpjvKo"
      },
      "outputs": [],
      "source": [
        "def save_fintuned_model(task,trainer,vocab,tok,farasa):\n",
        "  trainer.save_model(FINTUNED_MODEL+\"{}/xlm-roberta-{}-{}/{}\".format(task,farasa,vocab , tok))\n",
        "  print(\"the model saved in {}\".format(FINTUNED_MODEL+\"{}/xlm-roberta-{}-{}/{}\".format(task,farasa,vocab , tok)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0L2kw26NGey"
      },
      "outputs": [],
      "source": [
        "def save_logs(task,trainer,vocab,tok,farasa):\n",
        "  df = pd.DataFrame({\"vocab size\": [vocab], \"tokenization\": [tok],\"farasa\": [farasa]})\n",
        "  df1 = pd.DataFrame(trainer.state.log_history)\n",
        "  df2 = pd.DataFrame(trainer.evaluate(),index=[\"x\"])\n",
        "  data = [df, df1,df2]\n",
        "  df3 = pd.concat(data, ignore_index=True, sort=False)\n",
        "  df3.to_excel(FINTUNED_MODEL+\"{}/xlm-roberta-{}-{}/{}_paramaters.xlsx\".format(task,farasa,vocab , tok))\n",
        "  print(\"the model saved in {}\".format(FINTUNED_MODEL+\"{}/xlm-roberta-{}-{}/{}\".format(task,farasa,vocab , tok)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q4rJJw6zQsa"
      },
      "source": [
        "### loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6KsGi8CxCEM"
      },
      "outputs": [],
      "source": [
        "for farasa in is_farasa:\n",
        "  i = 0\n",
        "  for task in tasks:\n",
        "    dataset = load_datasets(task)\n",
        "    num_label = len(dataset.features['label'].names)\n",
        "    for tok in token_methods:\n",
        "      for vocab in vocab_sizes:\n",
        "        finetune_model = FINTUNED_MODEL+\"{}/xlm-roberta-{}-{}/{}\".format(task,farasa,vocab , tok)\n",
        "        if os.path.exists(finetune_model):\n",
        "          print(\"FINETUNED-Model\", finetune_model, \" already exists. Skipping...\")\n",
        "          print(\"------------------------------------------------\")\n",
        "          continue\n",
        "        model_path = PRETRAIND_MODEL+\"xlm-roberta-{}-{}/{}\".format(farasa,vocab , tok)\n",
        "        token_path = TOKENIZERS+\"xlm-roberta-{}-tokenizer-{}-{}.json\".format(tok , vocab,farasa)\n",
        "        if not os.path.exists(model_path):\n",
        "          print(f\"Model using {tok} NOT exists. Skipping...\")\n",
        "          print(\"------------------------------------------------\")\n",
        "          continue\n",
        "        print(model_path , token_path)\n",
        "        model , tokenizer , data_collator = load_model_tokenizer(model_path ,token_path ,num_label)\n",
        "        tokenized_dataset = tokenization(dataset,task)\n",
        "        train_test_eval = split_dataset(tokenized_dataset)\n",
        "        args = load_args(ARGS[i][0] , ARGS[i][1])\n",
        "        trainer = load_trainer(model,args , data_collator , train_test_eval)\n",
        "        trainer.train()\n",
        "        save_fintuned_model(task,trainer,vocab,tok,farasa)\n",
        "        save_logs(task,trainer,vocab,tok,farasa)\n",
        "        print(\"================================================================\")\n",
        "    i=i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWfOUHDmrKU4"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JYGxHNyrs0q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# push to hub"
      ],
      "metadata": {
        "id": "lNF6EQuzqwQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9g32Ht0q859",
        "outputId": "f0dd49a2-b300-416f-b74b-36dd00f96fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "farasa = \"nofarasa\"\n",
        "vocab = 16000\n",
        "tok = \"BPE\""
      ],
      "metadata": {
        "id": "vAd5oJ5krm9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = PRETRAIND_MODEL+\"xlm-roberta-{}-{}/{}\".format(farasa,vocab , tok)\n",
        "token_path = TOKENIZERS+\"xlm-roberta-{}-tokenizer-{}-{}.json\".format(tok , vocab,farasa)"
      ],
      "metadata": {
        "id": "VM2RRc8Rrjfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "tokenizer = Tokenizer.from_file(token_path)\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,pad_token = \"<pad>\")\n",
        "# ===================================================\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "QwCvymcYsID3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the model and tokenizer to Hugging Face\n",
        "model.push_to_hub(\"nourmorsy/arabic-llm-tokenizers_1\")\n",
        "tokenizer.push_to_hub(\"nourmorsy/arabic-llm-tokenizers_1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "f65d267d7b064d4e9232cb51bf9a61ab",
            "68f032c6d21c46258afd7323968b76d7",
            "5c272cc28cd24e26955a375a2b3c0b10",
            "6069685758b848959acc29983dda8de6",
            "738004f6247e4fe4975d1d52ac364e91",
            "9991fa20bfb14241b4791133312083b6",
            "4ba8ee8755ca4a43ba892f0ccbf00f1d",
            "ec51a2f226fe4885a3921f6daab9dc7a",
            "25033f3109dc43c9985bc06ca03f4136",
            "7cefc561515443c3a7dc506ef638f9e9",
            "8b8db12109a944eabd4d4965df9641c0"
          ]
        },
        "id": "OtFKdAyos9DX",
        "outputId": "12796878-e7bb-4afc-fe10-4201817b96bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/136M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f65d267d7b064d4e9232cb51bf9a61ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/nourmorsy/arabic-llm-tokenizers_1/commit/e6d6df891f2544c12165469e91e4de5146689363', commit_message='Upload tokenizer', commit_description='', oid='e6d6df891f2544c12165469e91e4de5146689363', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the file path for the output\n",
        "output_file_path = \"/content/output.txt\"  # Replace with your desired file path\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    for farasa in is_farasa:\n",
        "        for tok in token_methods:\n",
        "            for vocab in vocab_sizes:\n",
        "                model_path = PRETRAIND_MODEL + \"xlm-roberta-{}-{}/{}\".format(farasa, vocab, tok)\n",
        "                token_path = TOKENIZERS + \"xlm-roberta-{}-tokenizer-{}-{}.json\".format(tok, vocab, farasa)\n",
        "                if not os.path.exists(model_path):\n",
        "                    # output_file.write(f\"Model using {tok} NOT exists. Skipping...\\n\")\n",
        "                    # output_file.write(\"------------------------------------------------\\n\")\n",
        "                    continue\n",
        "\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "                output_file.write(\"-------------------------------start-------------------------------------\\n\")\n",
        "                output_file.write(\"xlm-roberta-{}-{}/{}\".format(farasa, vocab, tok) + \"\\n\")\n",
        "                output_file.write(\"--------------------------------------------------------------------\\n\")\n",
        "                config = model.config\n",
        "                output_file.write(\"Layers: {}\\n\".format(config.num_hidden_layers))\n",
        "                output_file.write(\"Heads: {}\\n\".format(config.num_attention_heads))\n",
        "                output_file.write(\"Max Length: {}\\n\".format(config.max_position_embeddings))\n",
        "                output_file.write('No of parameters: {}\\n'.format(model.num_parameters()))\n",
        "                output_file.write(f'Hardware: A100 GPU\\n')\n",
        "                output_file.write(f'Train data: 9.60GB\\n')\n",
        "                output_file.write(f'vocab_size: {vocab}\\n')\n",
        "                output_file.write(f'farsa: {farasa}\\n')\n",
        "                output_file.write(\"--------------------------------------------------------------------\\n\")\n",
        "                output_file.write(\"--------------------------------end------------------------------------\\n\")\n",
        "\n",
        "# Print the file path for reference\n",
        "print(\"Output written to:\", output_file_path)\n"
      ],
      "metadata": {
        "id": "lnzmJY51TJuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for farasa in is_farasa:\n",
        "  for tok in token_methods:\n",
        "    for vocab in vocab_sizes:\n",
        "      model_path = PRETRAIND_MODEL+\"xlm-roberta-{}-{}/{}\".format(farasa,vocab , tok)\n",
        "      token_path = TOKENIZERS+\"xlm-roberta-{}-tokenizer-{}-{}.json\".format(tok , vocab,farasa)\n",
        "      if not os.path.exists(model_path):\n",
        "          print(f\"Model using {tok} NOT exists. Skipping...\")\n",
        "          print(\"------------------------------------------------\")\n",
        "          continue\n",
        "      # ===================================================\n",
        "      # tokenizer = Tokenizer.from_file(token_path)\n",
        "      # tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,pad_token = \"<pad>\")\n",
        "      # ===================================================\n",
        "      model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "      # ===================================================\n",
        "      print(\"-------------------------------start-------------------------------------\")\n",
        "      print(model_path)\n",
        "      print(\"--------------------------------------------------------------------\")\n",
        "      config = model.config\n",
        "      print(\"Layers:\", config.num_hidden_layers)\n",
        "      print(\"Heads:\", config.num_attention_heads)\n",
        "      print(\"Max Length:\", config.max_position_embeddings)\n",
        "      print('No of parameters: ', model.num_parameters())\n",
        "      print(\"--------------------------------------------------------------------\")\n",
        "      print(\"--------------------------------end------------------------------------\")\n",
        "      # Push the model and tokenizer to Hugging Face\n",
        "      # if farasa == \"with-farasa\" : name = \"Farasa\"\n",
        "      # else: name = \"NoFarasa\"\n",
        "      # model.push_to_hub(f\"nourmorsy/PermoBERT-{name}-{tok}-{vocab}Token\")\n",
        "      # tokenizer.push_to_hub(f\"nourmorsy/PermoBERT-{name}-{tok}-{vocab}Token\")"
      ],
      "metadata": {
        "id": "J3rCh53auEqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for farasa in is_farasa:\n",
        "  for task in tasks:\n",
        "    for tok in token_methods:\n",
        "      for vocab in vocab_sizes:\n",
        "        finetune_model = FINTUNED_MODEL+\"{}/xlm-roberta-{}-{}/{}\".format(task,farasa,vocab , tok)\n",
        "        token_path = TOKENIZERS+\"xlm-roberta-{}-tokenizer-{}-{}.json\".format(tok , vocab,farasa)\n",
        "        if not os.path.exists(finetune_model):\n",
        "            print(f\"Model using {tok} NOT exists. Skipping...\")\n",
        "            print(\"------------------------------------------------\")\n",
        "            continue\n",
        "        # # ===================================================\n",
        "        tokenizer = Tokenizer.from_file(token_path)\n",
        "        tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,pad_token = \"<pad>\")\n",
        "        # # ===================================================\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(finetune_model)\n",
        "        # # ===================================================\n",
        "        # # Push the model and tokenizer to Hugging Face\n",
        "        if farasa == \"with-farasa\" : name = \"Farasa\"\n",
        "        else: name = \"NoFarasa\"\n",
        "        model.push_to_hub(f\"nourmorsy/PermoBERT-{task}-{name}-{tok}-{vocab}Token\")\n",
        "        tokenizer.push_to_hub(f\"nourmorsy/PermoBERT-{task}-{name}-{tok}-{vocab}Token\")"
      ],
      "metadata": {
        "id": "Kv3qxqgbyzA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"asafaya/bert-medium-arabic\")\n",
        "print('No of parameters: ', model.num_parameters())\n",
        "# return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC1JsBomMe_0",
        "outputId": "b69981f2-47ec-43d4-c438-57c277d1ebd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of parameters:  42162944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config"
      ],
      "metadata": {
        "id": "Uky8ccMHNW7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOGRVj7SNsUt",
        "outputId": "9ddd1637-be29-4880-d9d7-13bb0dbfdf5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"_name_or_path\": \"asafaya/bert-medium-arabic\",\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Layers:\", config.num_hidden_layers)\n",
        "print(\"Heads:\", config.num_attention_heads)\n",
        "# print(\"Batch Size:\", trainer.args.per_device_train_batch_size)\n",
        "print(\"Max Length:\", config.max_position_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzqvS0epNeGx",
        "outputId": "86bf3f6c-e346-4f38-9770-fc6ce7a22fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 8\n",
            "Heads: 8\n",
            "Max Length: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#calculate ratio"
      ],
      "metadata": {
        "id": "sdM1zZKi15Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install farasapy\n",
        "!pip install arabert\n",
        "from arabert import ArabertPreprocessor"
      ],
      "metadata": {
        "id": "D1H5HyPb_vkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2ac0db-87c7-414c-9c09-233a3d7beee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2023.11.17)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Collecting arabert\n",
            "  Downloading arabert-1.0.1-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyArabic (from arabert)\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: farasapy in /usr/local/lib/python3.10/dist-packages (from arabert) (0.0.14)\n",
            "Collecting emoji==1.4.2 (from arabert)\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (4.66.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic->arabert) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2023.11.17)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186460 sha256=9b0bc801fed3e5851b34a4ff6e789a05b09e853538b5212278124b5a4a94fdc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, PyArabic, arabert\n",
            "Successfully installed PyArabic-0.6.15 arabert-1.0.1 emoji-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"bert-base-arabert\"\n",
        "arabert_prep_0 = ArabertPreprocessor(\n",
        "    model_name= model_name,\n",
        "    keep_emojis = False,\n",
        "    remove_html_markup = True,\n",
        "    replace_urls_emails_mentions = True,\n",
        "    strip_tashkeel = True,\n",
        "    strip_tatweel = True,\n",
        "    insert_white_spaces = True,\n",
        "    remove_non_digit_repetition = True,\n",
        "    replace_slash_with_dash = None,\n",
        "    map_hindi_numbers_to_arabic = True,\n",
        "    apply_farasa_segmentation = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6GJ5bBM_q3r",
        "outputId": "6d238900-e4f4-46b4-fda6-33ea5db65b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:The selected model_name requires Farasa pre-segmentation, but apply_farasa_segmentation was set to False!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data):\n",
        "  for i in range(0,len(data)):\n",
        "    data[i]= arabert_prep_0.preprocess(data[i])\n",
        "  return data"
      ],
      "metadata": {
        "id": "I6A8o1-N_zo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Set the file path for the output\n",
        "output_file_path = \"/content/output.txt\"  # Replace with your desired file path\n",
        "ratio_unknow_to_know_path = \"/content/stats.txt\"\n",
        "# Open the file in write mode\n",
        "with open(ratio_unknow_to_know_path, \"w\") as ratio_unknow_to_know:\n",
        "  with open(output_file_path, \"w\") as output_file:\n",
        "    for farasa in is_farasa:\n",
        "      for task in tasks:\n",
        "        dataset = load_datasets(task)\n",
        "        output_file.write(f'LOADING DATASET :{task}\\n')\n",
        "        ratio_unknow_to_know.write(f'LOADING DATASET :{task}\\n')\n",
        "        text_columns = [column for column in dataset.column_names if column!='label']\n",
        "        all_text = [str(text) for column in text_columns for text in dataset[column]]\n",
        "        all_text = preprocessing(all_text)\n",
        "        unique_words = set(word for text in all_text for word in text.split())\n",
        "        unique_words_list = list(unique_words)\n",
        "        word_counts = Counter(word for text in all_text for word in text.split())\n",
        "        for tok in token_methods:\n",
        "          for vocab in vocab_sizes:\n",
        "            unknown_token_count = 0\n",
        "            know_token_count = 0\n",
        "            unknow_list = []\n",
        "            token_path = TOKENIZERS+\"xlm-roberta-{}-tokenizer-{}-{}.json\".format(tok , vocab,farasa)\n",
        "            if not os.path.exists(token_path):\n",
        "                # print(f\"Token NOT exists. Skipping...\")\n",
        "                # print(\"------------------------------------------------\")\n",
        "                continue\n",
        "            # # ===================================================\n",
        "            tokenizer = Tokenizer.from_file(token_path)\n",
        "            tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,pad_token = \"<pad>\")\n",
        "            # # ===================================================\n",
        "            for i in range(0,len(unique_words_list)):\n",
        "              encoded_input = tokenizer.tokenize(unique_words_list[i])\n",
        "              if(\"<unk>\" in encoded_input):\n",
        "                unknown_token_count+=1\n",
        "                unknow_list.append(unique_words_list[i])\n",
        "              else:\n",
        "                know_token_count +=1\n",
        "            # # ===================================================\n",
        "            ratio_unknow_to_know.write(\"================================================\\n\")\n",
        "            ratio_unknow_to_know.write(f\"token:{tok} , vocab:{vocab} , farasa:{farasa}\\n\")\n",
        "            ratio_unknow_to_know.write(f\"unknown_token_count:{unknown_token_count}\\nknow_token_count{know_token_count}\\n\")\n",
        "            ratio_unknow_to_know.write(f\"ratio unknow to know tokens:{unknown_token_count/know_token_count}\\n\")\n",
        "            # =======================================================\n",
        "            output_file.write(f\"token:{tok} , vocab:{vocab} , farasa:{farasa}\\n\")\n",
        "            output_file.write(f\"unknow_list:{unknow_list}\\n\")\n",
        "            output_file.write(\"===================================================================\\n\")\n"
      ],
      "metadata": {
        "id": "40W98muY17L6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XAOtc-peba4D",
        "kuUY4ZQtdScc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f65d267d7b064d4e9232cb51bf9a61ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68f032c6d21c46258afd7323968b76d7",
              "IPY_MODEL_5c272cc28cd24e26955a375a2b3c0b10",
              "IPY_MODEL_6069685758b848959acc29983dda8de6"
            ],
            "layout": "IPY_MODEL_738004f6247e4fe4975d1d52ac364e91"
          }
        },
        "68f032c6d21c46258afd7323968b76d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9991fa20bfb14241b4791133312083b6",
            "placeholder": "​",
            "style": "IPY_MODEL_4ba8ee8755ca4a43ba892f0ccbf00f1d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "5c272cc28cd24e26955a375a2b3c0b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec51a2f226fe4885a3921f6daab9dc7a",
            "max": 135805106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25033f3109dc43c9985bc06ca03f4136",
            "value": 135805106
          }
        },
        "6069685758b848959acc29983dda8de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cefc561515443c3a7dc506ef638f9e9",
            "placeholder": "​",
            "style": "IPY_MODEL_8b8db12109a944eabd4d4965df9641c0",
            "value": " 136M/136M [00:03&lt;00:00, 34.4MB/s]"
          }
        },
        "738004f6247e4fe4975d1d52ac364e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9991fa20bfb14241b4791133312083b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba8ee8755ca4a43ba892f0ccbf00f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec51a2f226fe4885a3921f6daab9dc7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25033f3109dc43c9985bc06ca03f4136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cefc561515443c3a7dc506ef638f9e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8db12109a944eabd4d4965df9641c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}