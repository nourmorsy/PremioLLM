# ArabicLLM Tokenization Strategies

## Project Overview and Goal
This study investigates the effectiveness of various tokenization strategies, specifically targeting the challenges of Arabic language processing. Our goal is to optimize tokenization for Arabic NLP tasks by comparing four tokenization methods—Byte Pair Encoding (BPE), WordPiece, BPE with Farasa (a morphological analyzer), and Word-level tokenization—across three vocabulary sizes.


## Citation

If you use this repository, please cite the following paper:
```
@{
  author = {Mohamed Taher Alrefaie and Nour Eldin Morsy and Nada Samir},
  title = {Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models},
  journal = {arXiv preprint arXiv:2403.11130},
  year = {2024}
}
```
